<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=content-language content="en">
<meta name=color-scheme content="light dark">
<meta name=author content="Arief Koesdwiady">
<meta name=description content="In this project, we develop a data-driven driver distraction detection system, which consists of 3 major development stages:
 Offline Multiview Classification Offline End-to-End Deep Learning Real-time End-to-End Deep Learning  The ultimate goal is to detect when a driver is distracted, i.e., doing activity other than driving such as texting, talking on the phone, etc, so that we can provide appropriate warning to the driver to stop him/her from being distracted.">
<meta name=keywords content="blog,developer,personal">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Data-Driven Driver Distraction Detection">
<meta name=twitter:description content="In this project, we develop a data-driven driver distraction detection system, which consists of 3 major development stages:
 Offline Multiview Classification Offline End-to-End Deep Learning Real-time End-to-End Deep Learning  The ultimate goal is to detect when a driver is distracted, i.e., doing activity other than driving such as texting, talking on the phone, etc, so that we can provide appropriate warning to the driver to stop him/her from being distracted.">
<meta property="og:title" content="Data-Driven Driver Distraction Detection">
<meta property="og:description" content="In this project, we develop a data-driven driver distraction detection system, which consists of 3 major development stages:
 Offline Multiview Classification Offline End-to-End Deep Learning Real-time End-to-End Deep Learning  The ultimate goal is to detect when a driver is distracted, i.e., doing activity other than driving such as texting, talking on the phone, etc, so that we can provide appropriate warning to the driver to stop him/her from being distracted.">
<meta property="og:type" content="article">
<meta property="og:url" content="http://abkoesdw.github.io/project_items/distraction/"><meta property="article:section" content="project_items">
<title>
Data-Driven Driver Distraction Detection · ariefkoesdwiady
</title>
<link rel=canonical href=http://abkoesdw.github.io/project_items/distraction/>
<link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin>
<link rel=stylesheet href=/css/coder.min.d9fddbffe6f27e69985dc5fe0471cdb0e57fbf4775714bc3d847accb08f4a1f6.css integrity="sha256-2f3b/+byfmmYXcX+BHHNsOV/v0d1cUvD2Eesywj0ofY=" crossorigin=anonymous media=screen>
<link rel=stylesheet href=/css/coder-dark.min.002ee2378e14c7a68f1f0a53d9694ed252090987c4e768023fac694a4fc5f793.css integrity="sha256-AC7iN44Ux6aPHwpT2WlO0lIJCYfE52gCP6xpSk/F95M=" crossorigin=anonymous media=screen>
<link rel=icon type=image/png href=/%20/images/favicon-32x32.png sizes=32x32>
<link rel=icon type=image/png href=/%20/images/favicon-16x16.png sizes=16x16>
<link rel=apple-touch-icon href=/%20/images/apple-touch-icon.png>
<link rel=apple-touch-icon sizes=180x180 href=/%20/images/apple-touch-icon.png>
<meta name=generator content="Hugo 0.92.1">
</head>
<body class="preload-transitions colorscheme-auto">
<div class=float-container>
<a id=dark-mode-toggle class=colorscheme-toggle>
<i class="fa fa-adjust fa-fw" aria-hidden=true></i>
</a>
</div>
<main class=wrapper>
<nav class=navigation>
<section class=container>
<a class=navigation-title href=/>
ariefkoesdwiady
</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle>
<i class="fa fa-bars fa-fw" aria-hidden=true></i>
</label>
<ul class=navigation-list>
<li class=navigation-item>
<a class=navigation-link href=/about/>About</a>
</li>
<li class=navigation-item>
<a class=navigation-link href=/publications/>Publications</a>
</li>
<li class=navigation-item>
<a class=navigation-link href=/projects/>Projects</a>
</li>
<li class=navigation-item>
<a class=navigation-link href=/posts/>Blog</a>
</li>
<li class=navigation-item>
<a class=navigation-link href=/contact/>Contact me</a>
</li>
</ul>
</section>
</nav>
<div class=content>
<section class="container page">
<article>
<header>
<h1 class=title>
<a class=title-link href=http://abkoesdw.github.io/project_items/distraction/>
Data-Driven Driver Distraction Detection
</a>
</h1>
</header>
<p>In this project, we develop a data-driven driver distraction detection system, which consists of 3 major development stages:</p>
<ul>
<li>Offline Multiview Classification</li>
<li>Offline End-to-End Deep Learning</li>
<li>Real-time End-to-End Deep Learning</li>
</ul>
<p>The ultimate goal is to detect when a driver is distracted, i.e., doing activity other than driving such as texting, talking on the phone, etc, so that we can provide appropriate warning to the driver to stop him/her from being distracted.</p>
<h3 id=driver-inattention-detection-system-a-pso-based-multiview-classification-approach>
Driver Inattention Detection System: A PSO-based Multiview Classification Approach
<a class=heading-link href=#driver-inattention-detection-system-a-pso-based-multiview-classification-approach>
<i class="fa fa-link" aria-hidden=true></i>
</a>
</h3>
<p>In this stage, we use two types of sensors to measure the states of the driver: camera and pressure sensors. The objective is to include more information to produce greater accuracy and reliability. The overal approach is illustrated by the following figure:</p>
<div style=text-align:center>
<img src=/images/distract.png width=95% height=95%>
</div>
<p>The camera is positioned such that the upper-body and hand position of the driver are captured. To to detect the griping force and position of the driver&rsquo;s hand, four pressure sensors are located on the steering wheel at 10-2 o&rsquo;clock position. Three paper-thin pressure sensors are attached to the right-top, left-top, and middle-bottom of the backrest to capture the driver&rsquo;s body movements.</p>
<div style=text-align:center>
<img src=/images/disdata.png width=75% height=75%>
</div>
<p>The experiment is carried out by participants with different ethnicities, genders, and ages. While driving in a driving simulator, we ask the participants to perform normal and distracted driving behavior.</p>
<div style=text-align:center>
<img src=/images/participant.png width=75% height=75%>
</div>
<hr>
<p><strong>N.B.</strong> The details of the work can be seen in this paper: <em>Arief Koesdwiady, Ramzi Abdelmoula, Fakhri Karray, Mohamed Kamel. &ldquo;Driver Inattention Detection System: A PSO-based Multiview Classification Approach&rdquo;. 2015 IEEE 18th International Conference on Intelligent Transportation Systems (IEEE-ITS). (pp. 1624-1629). IEEE</em>. <a href=https://ieeexplore.ieee.org/abstract/document/7313356>[link]</a></p>
<h3 id=end-to-end-deep-learning-for-driver-distraction-recognition>
End-to-End Deep Learning for Driver Distraction Recognition
<a class=heading-link href=#end-to-end-deep-learning-for-driver-distraction-recognition>
<i class="fa fa-link" aria-hidden=true></i>
</a>
</h3>
<p>In the second stage of the project, we develop an end-to-end deep neural network (DNN) for driver distraction detection, which is motivated by the performance of DNNs in other applications. Our machine learning pipeline is illustrated in the following diagram:</p>
<div style=text-align:center>
<img src=/images/distraction.png width=75%>
</div>
<p>We use only a video camera to capture the driver states. The camera is located such that the upper- body, hand positions, and rear-part of the car are captured and available to analyze. From the camera, sets of 640 × 480 - RGB video images with a frame rate 15 frames per second are obtained. In this experiment, two different cars are used in different lighting conditions. Ten drivers were involved in the experiments. Each driver was asked to perform or mimick the following driving activities: Normal/safe driving, Text messaging, Phone calling, Operating radio/navigation systems, etc.</p>
<div style=text-align:center>
<img src=/images/disdata2.png width=75%>
</div>
<p>Despite the challenging aspects considered in the dataset in terms of different illumination conditions, camera positions and variations in driver’s ethnicity, and genders, the proposed end-to-end framework was able to detect different classes with a best test accuracy of 95% and an average accuracy of 80% per class.</p>
<hr>
<p><strong>N.B.</strong> The details of the work can be seen in this paper: <em>Arief Koesdwiady, Safaa M Bedawi, Chaojie Ou, Fakhri Karray. &ldquo;End-to-end Deep Learning for Driver Distraction Recognition&rdquo;. 2017 International Conference Image Analysis and Recognition (ICIAR). (pp. 11-18). Springer, Cham</em>. <a href=https://link.springer.com/chapter/10.1007/978-3-319-59876-5_2>[link]</a></p>
<h3 id=real-time-end-to-end-deep-learning>
Real-Time End-to-End Deep Learning
<a class=heading-link href=#real-time-end-to-end-deep-learning>
<i class="fa fa-link" aria-hidden=true></i>
</a>
</h3>
<p>At this stage, we try to test the real-time performance of our system under new environment. Our work has been covered by several media outlets, one of them is a news station in Toronto (see the picture below).</p>
<div style=text-align:center>
<img src=/images/online.png width=75%>
</div>
</article>
</section>
</div>
<footer class=footer>
<section class=container>
©
2022
Arief Koesdwiady
·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.
</section>
</footer>
</main>
<script src=/js/coder.min.39a51230dce2ac866c049b52573e38bf60666af4bc63c1bdf203b9b2d95b1cd6.js integrity="sha256-OaUSMNzirIZsBJtSVz44v2BmavS8Y8G98gO5stlbHNY="></script>
</body>
</html>